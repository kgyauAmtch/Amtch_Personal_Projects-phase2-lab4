import argparse # Import argparse for command-line arguments
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, sum, count, avg, max, min, countDistinct, round
)

# --- Main execution block for command-line arguments ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Spark Job 2: KPI Calculation.")
    parser.add_argument('--data_source', type=str, required=True,
                        help='Base URL for transformed input data')
    parser.add_argument('--output_url', type=str, required=True,
                        help='Base URL for analytics output data ')
    args = parser.parse_args()

    # Initialize Spark Session
    # For EMR, Spark handles AWS SDKs, so no need for extraClassPath config here.
    spark = SparkSession.builder \
        .appName("Job2_AllKPIsCalculation") \
        .getOrCreate()


    # --- Input Paths (Reading from the transformed data saved by Job 1) ---
    # These paths are now derived from args.data_source
    transformed_vehicles_path = f"{args.data_source}vehicles/"
    transformed_locations_path = f"{args.data_source}locations/"
    transformed_users_path = f"{args.data_source}users/"
    transformed_rentals_path = f"{args.data_source}rentals/"

    # --- Output Paths for Analytics Metrics ---
    # These paths are now derived from args.output_url
    output_location_metrics_path = f"{args.output_url}location_metrics/"
    output_vehicle_type_metrics_path = f"{args.output_url}vehicle_type_metrics/"
    output_daily_metrics_path = f"{args.output_url}daily_metrics/"
    output_user_metrics_path = f"{args.output_url}user_metrics/"
    output_overall_transactions_path = f"{args.output_url}overall_transactions/"
    output_uniquevehiclelocation_path = f"{args.output_url}unique_vehicle/"
    


    print("--- Starting Job 2: All KPI Calculation and Metrics Storage ---")

    # --- Load Transformed Data (from Parquet files generated by Job 1) ---
    print("\nLoading transformed data from Parquet files...")
    df_vehicles_transformed = None
    df_locations_transformed = None
    df_users_transformed = None
    df_rentals_transformed = None

    try:
        df_vehicles_transformed = spark.read.parquet(transformed_vehicles_path).cache()
        print(f"Loaded df_vehicles_transformed from {transformed_vehicles_path}.")
    except Exception as e:
        print(f"Error loading transformed vehicles data from {transformed_vehicles_path}: {e}")

    try:
        df_locations_transformed = spark.read.parquet(transformed_locations_path).cache()
        print(f"Loaded df_locations_transformed from {transformed_locations_path}.")
    except Exception as e:
        print(f"Error loading transformed locations data from {transformed_locations_path}: {e}")

    try:
        df_users_transformed = spark.read.parquet(transformed_users_path).cache()
        print(f"Loaded df_users_transformed from {transformed_users_path}.")
    except Exception as e:
        print(f"Error loading transformed users data from {transformed_users_path}: {e}")

    try:
        df_rentals_transformed = spark.read.parquet(transformed_rentals_path).cache()
        print(f"Loaded df_rentals_transformed from {transformed_rentals_path}.")
    except Exception as e:
        print(f"Error loading transformed rentals data from {transformed_rentals_path}: {e}")

    # Proceed only if all essential DataFrames are loaded
    if all([df_vehicles_transformed, df_locations_transformed, df_users_transformed, df_rentals_transformed]):

        # --- KPI Calculations for Job 1 Scope (Location and Vehicle Type Analysis) ---
        print("\n--- Calculating Location and Vehicle Type Metrics (Job 1 Scope) ---")

        # Join rentals with locations for location-based metrics
        df_rentals_with_locations = df_rentals_transformed.alias("r").join(
            df_locations_transformed.alias("l"),
            col("r.pickup_location") == col("l.location_id"),
            "inner"
        ).select(
            col("r.*"),
            col("l.location_id"), # FIX: Ensure location_id is selected after join
            col("l.location_name"),
            col("l.city"),
            col("l.state")
        ).cache()


        # 1.1 Revenue per location, Total transactions per location, Average/Max/Min transaction amounts
        print("\nCalculating location-based metrics...")
        location_metrics = df_rentals_with_locations.groupBy("location_id", "location_name", "city", "state") \
            .agg(
                round(sum("total_amount"), 2).alias("total_revenue_per_location"),
                count("rental_id").alias("total_transactions_per_location"),
                round(avg("total_amount"), 2).alias("average_transaction_amount_per_location"),
                round(max("total_amount"), 2).alias("max_transaction_amount_per_location"),
                round(min("total_amount"), 2).alias("min_transaction_amount_per_location")
            )
        print("Location Metrics:")
        location_metrics.show(truncate=False)

        # Write location metrics to Parquet
        print(f"Saving location metrics to {output_location_metrics_path}")
        location_metrics.write.mode("overwrite").parquet(output_location_metrics_path)


        # 1.2 Unique vehicles used at each location
        print("\nCalculating unique vehicles per location...")
        unique_vehicles_per_location = df_rentals_with_locations.groupBy("location_id", "location_name", "city", "state") \
            .agg(
                countDistinct("vehicle_id").alias("unique_vehicles_used_per_location")
            )
        print("Unique Vehicles per Location:")
        unique_vehicles_per_location.show(truncate=False)
        print(f"Saving unique vehicles per location metrics to {output_uniquevehiclelocation_path}unique_vehicles/")
        unique_vehicles_per_location.write.mode("overwrite").parquet(f"{output_uniquevehiclelocation_path}unique_vehicles/")


        # Join rentals with vehicles for vehicle type-based metrics
        df_rentals_with_vehicles = df_rentals_transformed.alias("r").join(
            df_vehicles_transformed.alias("v"),
            col("r.vehicle_id") == col("v.vehicle_id"),
            "inner"
        ).select(
            col("r.*"),
            col("v.brand"),
            col("v.vehicle_type")
        ).cache()

        # 1.3 Rental duration and revenue by vehicle type
        print("\nCalculating vehicle type-based metrics...")
        vehicle_type_metrics = df_rentals_with_vehicles.groupBy("vehicle_type", "brand") \
            .agg(
                round(sum("total_amount"), 2).alias("total_revenue_by_vehicle_type"),
                count("rental_id").alias("total_rentals_by_vehicle_type"),
                round(avg("rental_duration_hours"), 2).alias("average_rental_duration_hours_by_type"),
                round(max("rental_duration_hours"), 2).alias("max_rental_duration_hours_by_type"),
                round(min("rental_duration_hours"), 2).alias("min_rental_duration_hours_by_type")
            )
        print("Vehicle Type Metrics:")
        vehicle_type_metrics.show(truncate=False)

        # Write vehicle type metrics to Parquet
        print(f"Saving vehicle type metrics to {output_vehicle_type_metrics_path}")
        vehicle_type_metrics.write.mode("overwrite").parquet(output_vehicle_type_metrics_path)


        # --- KPI Calculations for Job 2 Scope (User and Transaction Analysis) ---
        print("\n--- Calculating User and Transaction Analysis Metrics (Job 2 Scope) ---")

        # 2.1 Total transactions per day and Revenue per day
        print("\nCalculating daily transaction metrics...")
        daily_metrics = df_rentals_transformed.groupBy("rental_date") \
            .agg(
                count("rental_id").alias("total_transactions_daily"),
                round(sum("total_amount"), 2).alias("total_revenue_daily"),
                round(avg("total_amount"), 2).alias("average_transaction_amount_daily"),
                round(max("total_amount"), 2).alias("max_transaction_amount_daily"),
                round(min("total_amount"), 2).alias("min_transaction_amount_daily")
            ) \
            .orderBy("rental_date")
        print("Daily Transaction Metrics:")
        daily_metrics.show(truncate=False)

        # Write daily metrics to Parquet
        print(f"Saving daily metrics to {output_daily_metrics_path}")
        daily_metrics.write.mode("overwrite").parquet(output_daily_metrics_path)


        # Join rentals with users for user-specific metrics
        df_rentals_with_users = df_rentals_transformed.alias("r").join(
            df_users_transformed.alias("u"),
            col("r.user_id") == col("u.user_id"),
            "inner"
        ).select(
            col("r.*"),
            col("u.first_name"),
            col("u.last_name"),
            col("u.email")
        ).cache()

        # 2.2 User-specific spending and rental duration metrics
        print("\nCalculating user-specific metrics...")
        user_metrics = df_rentals_with_users.groupBy("user_id", "first_name", "last_name", "email") \
            .agg(
                count("rental_id").alias("total_rentals_by_user"),
                round(sum("total_amount"), 2).alias("total_spending_by_user"),
                round(avg("total_amount"), 2).alias("average_transaction_by_user"),
                round(avg("rental_duration_hours"), 2).alias("average_rental_duration_hours_by_user"),
                round(max("rental_duration_hours"), 2).alias("max_rental_duration_hours_by_user"),
                round(min("rental_duration_hours"), 2).alias("min_rental_duration_hours_by_user")
            ) \
            .orderBy(col("total_spending_by_user").desc())
        print("User-Specific Metrics (Top 5):")
        user_metrics.show(5, truncate=False)

        # Write user-specific metrics to Parquet
        print(f"Saving user-specific metrics to {output_user_metrics_path}")
        user_metrics.write.mode("overwrite").parquet(output_user_metrics_path)


        # 2.3 Maximum and minimum transaction amounts (overall for rentals)
        print("\nCalculating overall max/min transaction amounts...")
        overall_min_max_transactions = df_rentals_transformed.agg(
            round(max("total_amount"), 2).alias("overall_max_transaction_amount"),
            round(min("total_amount"), 2).alias("overall_min_transaction_amount")
        )
        print("Overall Max/Min Transaction Amounts:")
        overall_min_max_transactions.show(truncate=False)

        # Write overall min/max transaction to Parquet (can be a single file)
        print(f"Saving overall min/max transaction amounts to {output_overall_transactions_path}")
        overall_min_max_transactions.write.mode("overwrite").parquet(output_overall_transactions_path)


        # Unpersist all cached DataFrames to free up memory
        # Check if DFs exist before unpersisting
        if df_vehicles_transformed: df_vehicles_transformed.unpersist()
        if df_locations_transformed: df_locations_transformed.unpersist()
        if df_users_transformed: df_users_transformed.unpersist()
        if df_rentals_transformed: df_rentals_transformed.unpersist()
        # Joined DFs
        if 'df_rentals_with_locations' in locals() and df_rentals_with_locations: df_rentals_with_locations.unpersist()
        if 'df_rentals_with_vehicles' in locals() and df_rentals_with_vehicles: df_rentals_with_vehicles.unpersist()
        if 'df_rentals_with_users' in locals() and df_rentals_with_users: df_rentals_with_users.unpersist()

    else:
        print("One or more essential transformed DataFrames could not be loaded. Aborting KPI calculation.")

    # Stop Spark Session
    spark.stop()
    print("\nJob 2: All KPI Calculation Complete. All metrics saved to Parquet folders.")