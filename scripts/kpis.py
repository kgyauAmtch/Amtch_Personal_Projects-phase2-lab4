import argparse # Import argparse for command-line arguments
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, sum, count, avg, max, min, countDistinct, round
)

# --- Main execution block for command-line arguments ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Spark Job 2: KPI Calculation.")
    parser.add_argument('--data_source', type=str, required=True,
                        help='Base URL for transformed input data')
    parser.add_argument('--output_url', type=str, required=True,
                        help='Base URL for analytics output data ')
    args = parser.parse_args()

    # Initialize Spark Session
    # For EMR, Spark handles AWS SDKs, so no need for extraClassPath config here.
    spark = SparkSession.builder \
        .appName("Job2_AllKPIsCalculation") \
        .getOrCreate()

    # --- Input Paths (Reading from the transformed data saved by Job 1) ---
    transformed_vehicles_path = f"{args.data_source}vehicles/"
    transformed_locations_path = f"{args.data_source}locations/"
    transformed_users_path = f"{args.data_source}users/"
    transformed_rentals_path = f"{args.data_source}rentals/"

    # --- Output Paths for Analytics Metrics (Reduced to 4) ---
    output_location_metrics_path = f"{args.output_url}location_metrics/"
    output_vehicle_type_metrics_path = f"{args.output_url}vehicle_type_metrics/"
    output_temporal_metrics_path = f"{args.output_url}temporal_metrics/"
    output_user_metrics_path = f"{args.output_url}user_metrics/"

    print("--- Starting Job 2: All KPI Calculation and Metrics Storage ---")

    # --- Load Transformed Data (from Parquet files generated by Job 1) ---
    print("\nLoading transformed data from Parquet files...")
    df_vehicles_transformed = None
    df_locations_transformed = None
    df_users_transformed = None
    df_rentals_transformed = None

    try:
        df_vehicles_transformed = spark.read.parquet(transformed_vehicles_path).cache()
        print(f"Loaded df_vehicles_transformed from {transformed_vehicles_path}.")
    except Exception as e:
        print(f"Error loading transformed vehicles data from {transformed_vehicles_path}: {e}")

    try:
        df_locations_transformed = spark.read.parquet(transformed_locations_path).cache()
        print(f"Loaded df_locations_transformed from {transformed_locations_path}.")
    except Exception as e:
        print(f"Error loading transformed locations data from {transformed_locations_path}: {e}")

    try:
        df_users_transformed = spark.read.parquet(transformed_users_path).cache()
        print(f"Loaded df_users_transformed from {transformed_users_path}.")
    except Exception as e:
        print(f"Error loading transformed users data from {transformed_users_path}: {e}")

    try:
        df_rentals_transformed = spark.read.parquet(transformed_rentals_path).cache()
        print(f"Loaded df_rentals_transformed from {transformed_rentals_path}.")
    except Exception as e:
        print(f"Error loading transformed rentals data from {transformed_rentals_path}: {e}")

    # Proceed only if all essential DataFrames are loaded
    if all([df_vehicles_transformed, df_locations_transformed, df_users_transformed, df_rentals_transformed]):

        # --- OUTPUT 1: Location Metrics (Combined location stats + unique vehicles) ---
        print("\n--- Calculating Location Metrics ---")
        
        # Join rentals with locations for location-based metrics
        df_rentals_with_locations = df_rentals_transformed.alias("r").join(
            df_locations_transformed.alias("l"),
            col("r.pickup_location") == col("l.location_id"),
            "inner"
        ).select(
            col("r.*"),
            col("l.location_id"),
            col("l.location_name"),
            col("l.city"),
            col("l.state")
        ).cache()

        # Combined location metrics with unique vehicles count
        location_metrics = df_rentals_with_locations.groupBy("location_id", "location_name", "city", "state") \
            .agg(
                round(sum("total_amount"), 2).alias("total_revenue_per_location"),
                count("rental_id").alias("total_transactions_per_location"),
                round(avg("total_amount"), 2).alias("average_transaction_amount_per_location"),
                round(max("total_amount"), 2).alias("max_transaction_amount_per_location"),
                round(min("total_amount"), 2).alias("min_transaction_amount_per_location"),
                countDistinct("vehicle_id").alias("unique_vehicles_used_per_location")
            )
        print("Location Metrics:")
        location_metrics.show(truncate=False)

        # Write location metrics to Parquet
        print(f"Saving location metrics to {output_location_metrics_path}")
        location_metrics.write.mode("overwrite").parquet(output_location_metrics_path)

        # --- OUTPUT 2: Vehicle Type Metrics ---
        print("\n--- Calculating Vehicle Type Metrics ---")
        
        # Join rentals with vehicles for vehicle type-based metrics
        df_rentals_with_vehicles = df_rentals_transformed.alias("r").join(
            df_vehicles_transformed.alias("v"),
            col("r.vehicle_id") == col("v.vehicle_id"),
            "inner"
        ).select(
            col("r.*"),
            col("v.brand"),
            col("v.vehicle_type")
        ).cache()

        # Vehicle type metrics
        vehicle_type_metrics = df_rentals_with_vehicles.groupBy("vehicle_type", "brand") \
            .agg(
                round(sum("total_amount"), 2).alias("total_revenue_by_vehicle_type"),
                count("rental_id").alias("total_rentals_by_vehicle_type"),
                round(avg("rental_duration_hours"), 2).alias("average_rental_duration_hours_by_type"),
                round(max("rental_duration_hours"), 2).alias("max_rental_duration_hours_by_type"),
                round(min("rental_duration_hours"), 2).alias("min_rental_duration_hours_by_type")
            )
        print("Vehicle Type Metrics:")
        vehicle_type_metrics.show(truncate=False)

        # Write vehicle type metrics to Parquet
        print(f"Saving vehicle type metrics to {output_vehicle_type_metrics_path}")
        vehicle_type_metrics.write.mode("overwrite").parquet(output_vehicle_type_metrics_path)

        # --- OUTPUT 3: Temporal Metrics (Daily + Overall) ---
        print("\n--- Calculating Temporal Metrics ---")
        
        # Daily metrics
        daily_metrics = df_rentals_transformed.groupBy("rental_date") \
            .agg(
                count("rental_id").alias("total_transactions_daily"),
                round(sum("total_amount"), 2).alias("total_revenue_daily"),
                round(avg("total_amount"), 2).alias("average_transaction_amount_daily"),
                round(max("total_amount"), 2).alias("max_transaction_amount_daily"),
                round(min("total_amount"), 2).alias("min_transaction_amount_daily")
            ) \
            .orderBy("rental_date")

        # Overall metrics
        overall_metrics = df_rentals_transformed.agg(
            round(max("total_amount"), 2).alias("overall_max_transaction_amount"),
            round(min("total_amount"), 2).alias("overall_min_transaction_amount"),
            round(sum("total_amount"), 2).alias("overall_total_revenue"),
            count("rental_id").alias("overall_total_transactions"),
            round(avg("total_amount"), 2).alias("overall_average_transaction_amount")
        ).withColumn("metric_type", lit("overall"))

        # Add metric_type column to daily metrics for distinction
        daily_metrics_with_type = daily_metrics.withColumn("metric_type", lit("daily"))

        # Combine daily and overall metrics (note: schemas need to match for union)
        # We'll save them as separate datasets within the same output folder
        print("Daily Transaction Metrics:")
        daily_metrics.show(truncate=False)
        print("Overall Transaction Metrics:")
        overall_metrics.show(truncate=False)

        # Write temporal metrics to Parquet (daily and overall in subfolders)
        print(f"Saving temporal metrics to {output_temporal_metrics_path}")
        daily_metrics.write.mode("overwrite").parquet(f"{output_temporal_metrics_path}daily/")
        overall_metrics.write.mode("overwrite").parquet(f"{output_temporal_metrics_path}overall/")

        # --- OUTPUT 4: User Metrics ---
        print("\n--- Calculating User Metrics ---")
        
        # Join rentals with users for user-specific metrics
        df_rentals_with_users = df_rentals_transformed.alias("r").join(
            df_users_transformed.alias("u"),
            col("r.user_id") == col("u.user_id"),
            "inner"
        ).select(
            col("r.*"),
            col("u.first_name"),
            col("u.last_name"),
            col("u.email")
        ).cache()

        # User-specific metrics
        user_metrics = df_rentals_with_users.groupBy("user_id", "first_name", "last_name", "email") \
            .agg(
                count("rental_id").alias("total_rentals_by_user"),
                round(sum("total_amount"), 2).alias("total_spending_by_user"),
                round(avg("total_amount"), 2).alias("average_transaction_by_user"),
                round(max("rental_duration_hours"), 2).alias("max_rental_duration_hours_by_user"),
                round(min("rental_duration_hours"), 2).alias("min_rental_duration_hours_by_user")
            ) \
            .orderBy(col("total_spending_by_user").desc())
        print("User-Specific Metrics (Top 10):")
        user_metrics.show(10, truncate=False)

        # Write user-specific metrics to Parquet
        print(f"Saving user-specific metrics to {output_user_metrics_path}")
        user_metrics.write.mode("overwrite").parquet(output_user_metrics_path)

        # Unpersist all cached DataFrames to free up memory
        if df_vehicles_transformed: df_vehicles_transformed.unpersist()
        if df_locations_transformed: df_locations_transformed.unpersist()
        if df_users_transformed: df_users_transformed.unpersist()
        if df_rentals_transformed: df_rentals_transformed.unpersist()
        # Joined DFs
        if 'df_rentals_with_locations' in locals() and df_rentals_with_locations: df_rentals_with_locations.unpersist()
        if 'df_rentals_with_vehicles' in locals() and df_rentals_with_vehicles: df_rentals_with_vehicles.unpersist()
        if 'df_rentals_with_users' in locals() and df_rentals_with_users: df_rentals_with_users.unpersist()

    else:
        print("One or more essential transformed DataFrames could not be loaded. Aborting KPI calculation.")

    # Stop Spark Session
    spark.stop()
    print("\nJob 2: All KPI Calculation Complete. 4 consolidated metrics saved to Parquet folders.")
    print("Output structure:")
    print("1. location_metrics/ - Location performance + unique vehicles")
    print("2. vehicle_type_metrics/ - Vehicle type performance")
    print("3. temporal_metrics/ - Daily metrics + overall business metrics")
    print("4. user_metrics/ - User behavior and spending patterns")